# -*- coding: utf-8 -*-
"""Reto de aprendizaje 1: Predicción precio vivienda California.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uU1dc6r9ayFPOeLLiL6UdjzJBjPBr_zG

# **Predicción del precio de la vivienda en California (Scikit-Learn)**
##**DESCRIPCIÓN**

La Oficina del Censo de EE. UU. ha publicado los datos del censo de California, que tiene 10 tipos de métricas, como la población, el ingreso medio, el precio medio de la vivienda, etc., para cada grupo de bloques en California. El conjunto de datos también sirve como entrada para el alcance del proyecto e intenta especificar los requisitos funcionales y no funcionales para él.

Objetivo del problema: el proyecto tiene como objetivo construir un modelo de precios de viviendas para predecir el valor medio de las viviendas en California utilizando el conjunto de datos proporcionado. Este modelo debería aprender de los datos y poder predecir el precio medio de la vivienda en cualquier distrito, dadas todas las demás métricas.

Los distritos o grupos de bloques son las unidades geográficas más pequeñas para las que la Oficina del Censo de EE. UU. publica datos de muestra (un grupo de bloques suele tener una población de 600 a 3000 personas). Hay 20.640 distritos en el conjunto de datos del proyecto. *Dominio: Finance and Housing*.

**Descripción del conjunto de datos:**

Campo(): Descripción.
*   longitude (numérico con signo - float) : *valor de longitud para el bloque en California, EE. UU.*
*   latitude (numeric - float): valor de latitud para el bloque en California, EE. UU.
*   housing_median_age (numeric - int ) : Edad mediana de la casa en el bloque
*   total_rooms (numeric - int ) : Recuento del número total de habitaciones (excluyendo dormitorios) en todas las casas del bloque

*   total_bedrooms (numeric - float): recuento del número total de dormitorios en todas las casas del bloque
*   población (numérico - int): Recuento del número total de habitantes en el bloque
*   hogares (numérico - int): Recuento del número total de hogares en el bloque
*   mediana_ingreso (numérico - flotante): Mediana del ingreso familiar total de todas las casas en el bloque
*   ocean_proximity (numérico - categórico): Tipo de paisaje del bloque [Valores únicos: 'NEAR BAY', '<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'ISLAND' ]
*   median_house_value (numeric - int ) : Mediana de los precios de los hogares de todas las casas en el bloque.

## Importar Librerias:
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import scipy.stats as stats


from sklearn.model_selection import StratifiedShuffleSplit,train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score, classification_report
from sklearn.preprocessing import PowerTransformer

plt.tight_layout()

from warnings import filterwarnings
filterwarnings('ignore')

"""## Importar el conjunto de datos:"""

data = pd.read_csv('https://raw.githubusercontent.com/ElDiBu369/Prediccion-precio-vivienda-California-Scikit-Learn-/main/fetch_california_housing%20de%20Sklearn%20Datasets.csv')
data

"""## Trabajando en la copia del conjunto de datos:

"""

df = data.copy()
df

"""## Análisis básico:

"""

df.info()

df.shape

df.size

col = df.columns.to_list()
col

for i in col:
    print('The datatype of',i, 'is', df[i].dtypes)

"""## Resumen de estadísticas para columnas categóricas y numéricas:"""

num = df.select_dtypes(include=np.number).columns.to_list()
num

cat = df.select_dtypes(exclude=np.number).columns.to_list()
cat

df[num].describe()

df[cat].describe()

"""## Comprobando valores nulos:"""

df.isna().sum()

"""## Tratamiento de valores nulos:

"""

df['total_bedrooms'].isna().sum()

df['ocean_proximity'].unique()

a=round(df.groupby(['ocean_proximity'])['total_bedrooms'].mean()[1],0)
a

df["total_bedrooms"]=df["total_bedrooms"].fillna(a)

df['total_bedrooms'].isna().sum()

"""##Mapa de California construido con los datos de longitud y latitud"""

df.plot(kind='scatter',x='longitude',y='latitude',figsize=(10, 7))

df.plot(kind='scatter',x='longitude',alpha=0.1,y='latitude',figsize=(10, 7))

df.plot(kind='scatter',x='longitude',alpha=0.1,y='latitude',figsize=(10, 7),s=df['population']/100,c='median_house_value',colorbar=True)

df.plot(kind='scatter',x='longitude',alpha=0.4,y='latitude',figsize=(10, 7),s=df['population']/100,c='median_house_value',cmap=plt.get_cmap(),colorbar=True)

import folium
mapa=folium.Map(location=[37.50,-122.2],zoom_start=6)
mapa

distritos=df[['latitude','longitude']].to_numpy()
distritos

from folium import plugins
from folium.plugins import HeatMap
HeatMap(distritos, radius=15,max_val=0.1,min_opacity=0.4).add_to(mapa)
mapa

"""## Eliminar columnas no deseadas:

"""

col

df.drop(['longitude','latitude'],axis=1,inplace=True)

df

columns = df.columns.to_list()
columns

coll = ['housing_median_age',
 'total_rooms',
 'total_bedrooms',
 'population',
 'households',
 'median_income',
 'median_house_value']
coll

"""## Visualización de datos:

### Univariado:
"""

coll

cat

for i in columns:
    sns.histplot(df[i])
    plt.show()

for i in coll:
    sns.distplot(df[i])
    plt.show()

for i in coll:
    sns.boxplot(y=df[i])
    plt.show()

df[cat].value_counts()

sns.countplot(x='ocean_proximity', data=df)
plt.show()

df['ocean_proximity'].value_counts().plot(kind='bar')
plt.show()

"""**Dado que los datos de Island son datos muy inferiores en comparación con otros valores, seran eliminados**

## Eliminando los valores atípicos:
"""

dff = df[df['ocean_proximity'] != 'ISLAND']

dff

dff['ocean_proximity'].value_counts()

"""### Bivariado:"""

for i in coll:
    sns.boxplot(y=df[i],x=df['ocean_proximity'])
    plt.show()

c = ['households','total_rooms','total_bedrooms']

for i in c:
    sns.scatterplot(y=df[i],x=df['population'])
    plt.show()

ind = ['housing_median_age',
 'total_rooms',
 'total_bedrooms',
 'population',
 'households',
 'median_income']

dep = ['median_house_value']

for i in ind:
    sns.lmplot(data=dff, x=i , y='median_house_value')
    plt.show()

"""### Multivariado:"""

coll

sns.heatmap(dff[coll].corr(),annot=True)
plt.show()

"""* Las columnas total_rooms, total_bedrooms, population y household son las columnas que están altamente correlacionadas
* Esto se puede reducir utilizando el factor de inflación de varianza (VIF)
* La columna mediana_ingresos tiene una buena correlación con el objetivo

## Transformación:
"""

dff.skew()

dff.kurt()

"""Excepto la columna housing_median_age, todas las demás están muy sesgadas, por lo que debemos transformarla"""

trans = ['total_rooms',
 'total_bedrooms',
 'population',
 'households',
 'median_income']

from sklearn.preprocessing import PowerTransformer

p = PowerTransformer(method='box-cox')
dff[trans] = p.fit_transform(dff[trans])

dff

dff.isna().sum()

dff.skew()

"""## Codificación:"""

df1 = pd.get_dummies(data=dff, columns = cat,drop_first=True)

df1

"""## 1 Construcción del modelo:

"""

df1

x = df1.drop(['median_house_value'],axis=1)
y = df1['median_house_value']

from sklearn.model_selection import train_test_split

xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.3,random_state=100)

print(xtrain.shape)
print(ytrain.shape)
print(xtest.shape)
print(ytest.shape)

"""## 2 Modelo base:

"""

import statsmodels.api as sm
from sklearn.linear_model import LinearRegression,Lasso,Ridge
from sklearn.model_selection import GridSearchCV

xtrainc = sm.add_constant(xtrain)
xtestc = sm.add_constant(xtest)

basemodel = sm.OLS(ytrain,xtrainc).fit()
basemodel.summary()

"""* El cuadrado R es 0,6, es decir, el 60 % del objetivo puede explicarse mediante las variables independientes
* El valor p de todas las columnas es 0, que es menor que 0,05, por lo que todas las columnas son significativas
* Por lo tanto, también verificamos la multicolinealidad.

### Comprueba la multicolinealidad usando VIF
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF

col1 = xtrainc.columns.to_list()
col1

vif = [ VIF(xtrainc.values,i) for i in range(xtrainc.shape[1])]

vif_df = pd.DataFrame(vif,index=col1,columns=['VIF'])
vif_df.sort_values(by='VIF',ascending=False)

"""**Elimina la columna hogares y luego verifique el VIF**"""

xtrainc1 = xtrainc.drop('households',axis=1)
xtestc1 = xtestc.drop('households',axis=1)

col2 = xtrainc1.columns.to_list()
col2

vif = [ VIF(xtrainc1.values,i) for i in range(xtrainc1.shape[1])]

vif_df = pd.DataFrame(vif,index=col2,columns=['VIF'])
vif_df.sort_values(by='VIF',ascending=False)

"""**Elimine la columna total_rooms y luego verifique el VIF**"""

xtrainc2 = xtrainc1.drop('total_rooms',axis=1)
xtestc2 = xtestc1.drop('total_rooms',axis=1)

col3 = xtrainc2.columns.to_list()
col3

vif = [ VIF(xtrainc2.values,i) for i in range(xtrainc2.shape[1])]

vif_df = pd.DataFrame(vif,index=col3,columns=['VIF'])
vif_df.sort_values(by='VIF',ascending=False)

"""Todos los valores de VIF son menos dhan 5, luego podemos continuar con el proceso adicional"""

model2 = sm.OLS(ytrain,xtrainc2).fit()
model2.summary()

#### Metrices:

from sklearn.metrics import accuracy_score,r2_score,mean_absolute_error,mean_squared_error

def met(name,key,arg):
    print('Name of the model: ',name)
    print('R-square of the model:',r2_score(key,arg))
    print('RMSE of the model:',np.sqrt(mean_squared_error(key,arg)))
    print('MAE of the model:',mean_absolute_error(key,arg))

"""## 3 Modelo Sklearn:"""

lr = LinearRegression()

model3 = lr.fit(xtrainc2,ytrain)

ypred_tr= lr.predict(xtrainc2)
ypred_te= lr.predict(xtestc2)

met('Linear Regression model and result for Train',ytrain,ypred_tr)

lr.score(xtrainc2,ytrain)

met('Linear Regression model and result for Test',ytest,ypred_te)

lr.score(xtestc2,ytest)

"""## 4 Modelo usando RFE:"""

from sklearn.feature_selection import RFE

rfe = RFE(estimator=lr)

model4 = rfe.fit(xtrainc2,ytrain)

rank = list(rfe.ranking_)
rank

rfe_df = pd.DataFrame(rank,index=list(xtrainc2.columns))
rfe_df.sort_values(by=0)

"""## 5 Modelo con columnas de rango 1:"""

xtrainc3 = xtrainc2.drop(['ocean_proximity_NEAR BAY','ocean_proximity_NEAR OCEAN','housing_median_age','const'],axis=1)
xtestc3 = xtestc2.drop(['ocean_proximity_NEAR BAY','ocean_proximity_NEAR OCEAN','housing_median_age','const'],axis=1)

model5 = lr.fit(xtrainc3,ytrain)

ypred_rfe_tr = model5.predict(xtrainc3)
ypred_rfe_te = model5.predict(xtestc3)

met('RFE MODEL and result for train',ytrain,ypred_rfe_tr)

met('RFE MODEL and result for train',ytest,ypred_rfe_te)

"""## 6 Modelo usando Regularización:"""

ridge = Ridge()
model6 = ridge.fit(xtrainc3,ytrain)

ypred_rtr = ridge.predict(xtrainc3)
ypred_rte = ridge.predict(xtestc3)

met('Ridge model and result for train',ytrain,ypred_rtr)

met('Ridge model and result for test',ytest,ypred_rte)

"""## 7 Modelo utilizando el regresor del árbol de decisión:"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor

dt = DecisionTreeRegressor(random_state=100)

model7 = dt.fit(xtrainc2,ytrain)

ypred_dt_tr = dt.predict(xtrainc2)
ypred_dt_te = dt.predict(xtestc2)

met('Decision Tree model and result for train',ytrain,ypred_dt_tr)

met('Decision tree model and result for train',ytest,ypred_dt_te)

"""## 8 Modelo usando Bagging (Random Forest Regressor):"""

rf = RandomForestRegressor(random_state=100)

model8 = rf.fit(xtrainc2,ytrain)

ypred_rf_tr = rf.predict(xtrainc2)
ypred_rf_te = rf.predict(xtestc2)

met('Random Forest model and result for test',ytest,ypred_rf_te)

"""## Sintonización de hiperparámetros en el modelo 8:"""

params = {'n_estimators' : [70,80,90],
          'max_depth' : [3,4,5],
           'min_samples_split':[2,3,4],
            'min_samples_leaf': [2,3,4]}

grid = GridSearchCV(estimator= rf , param_grid=params, cv=3, scoring='r2')

grid.fit(xtrainc2,ytrain)

grid.best_estimator_

grid.best_params_

"""## Modelo utilizando parámetros sintonizados"""

rft = RandomForestRegressor(max_depth =5,min_samples_leaf = 2,min_samples_split = 2,n_estimators =80,random_state=100)

rft.fit(xtrainc2,ytrain)

ypred_rft_te =  rft.predict(xtestc2)

met('Tunned Random Forest model and result for test',ytest,ypred_rft_te)

"""El mejor modelo es Random Forest Regressor con un R-Cuadrado de 0.6315882256704293 equivalente al 63% del objetivo."""
